<!doctype html>
<html lang="en-us">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-1687066-2"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'UA-1687066-2');
  </script>
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css" rel="stylesheet" integrity="sha512-Evv84Mr4kqVGRNSgIGL/F/aIDqQb7xQ2vcrdIwxfjThSH8CSR7PBEakCr51Ck+w+/U6swU2Im1vVX0SVk9ABhg==" crossorigin="anonymous">
  <link href="http://localhost:8000/css/site.css" rel="stylesheet">
  <link rel="preload" href="http://localhost:8000/fonts/blockzone-webfont.woff2" as="font" type="font/woff2" crossorigin>
  
</head>
<body>
  <header id="preamble" class="status">
    <div class="logo">
      <a href="/">
        <img src="https://www.gravatar.com/avatar/3b67365812827fa25df5093b38934a8f?s=80" class="avatar" alt="My photo"/>
      </a>
      <a href="/"><h2>Duncan Mac-Vicar P.</h2></a>
      <div id="social">
        <a title="dmacvicar on Github" href="https://github.com/dmacvicar">
        <i class="fa-brands fa-github"></i>
      </a>
      <a title="dmacvicar on Hacker News" href="https://news.ycombinator.com/user?id=dmacvicar">
        <i class="fa-brands fa-hacker-news"></i>
      </a>
      <a title="dmacvicar on Twitter" href="https://twitter.com/dmacvicar">
        <i class="fa-brands fa-twitter"></i>
      </a>
      <a title="RSS feed" id="atom" href="posts/rss.xml">
        <i class="fa-solid fa-rss"></i>
      </a>
    </div>
   <sub>I am inspired by creating software and learning from others. When things break, I fix them or go play guitar <i class="fa-solid fa-guitar"></i> ...</sub>
 </header>
 <main id="main">
 
<h1>Raspberry Pi cluster with k3s &amp; Salt (Part 1)</h1>
<div class="date">07 Sep 2020</div>
<div class="content">
  <p>
I have been running some workloads on Raspberry Pi&rsquo;s / <a href="https://software.opensuse.org/distributions/leap">Leap</a> for some time. I manage them using <a href="https://docs.saltstack.com/en/latest/topics/ssh">salt-ssh</a> along with a <a href="https://www.kickstarter.com/projects/pine64/pine-a64-first-15-64-bit-single-board-super-comput/">Pine64</a> running <a href="https://www.openbsd.org">OpenBSD</a>. You can read more about using Salt this way in my <a href="../2016-05-18-using-salt-like-ansible/index.html">Using Salt like Ansible</a> post.
</p>


<figure id="org54203ed">
<img src="images/rpi-cluster-small.jpg" alt="rpi-cluster-small.jpg">

</figure>

<p>
The workloads ran on containers, which were managed with <a href="https://systemd.io/">systemd</a> and <a href="https://podman.io">podman</a>. Salt managed the systemd service files on <code>/etc/systemd</code>, which start, monitor and stops the containers. For example, the <code>homeassistant.sls</code> state, managed the service file for <a href="https://mosquitto.org">mosquitto</a>:
</p>

<div class="org-src-container">
<pre class="src src-yaml">homeassistant.mosquito.deploy:
  file.managed:
    - name: /root/config/eclipse-mosquitto/mosquitto.conf
    - source: salt://homeassistant/files/mosquitto/mosquitto.conf

homeassistant.eclipse-mosquitto.container.service:
  file.managed:
    - name: /etc/systemd/system/eclipse-mosquitto.service
    - contents: |
	[Unit]
	Description=%N Podman Container
	After=network.target

	[Service]
	Type=simple
	TimeoutStartSec=5m
	ExecStartPre=-/usr/bin/podman rm -f "%N"
	ExecStart=/usr/bin/podman run -ti --rm --name="%N" -p 1883:1883 -p 9001:9001 -v /root/config/eclipse-mosquitto:/mosquitto/config -v /etc/localtime:/etc/localtime:ro --net=host docker.io/library/eclipse-mosquitto
	ExecReload=-/usr/bin/podman stop "%N"
	ExecReload=-/usr/bin/podman rm "%N"
	ExecStop=-/usr/bin/podman stop "%N"
	Restart=on-failure
	RestartSec=30

	[Install]
	WantedBy=multi-user.target
  service.running:
    - name: eclipse-mosquitto
    - enable: True
    - require:
      - pkg: homeassistant.podman.pkgs
      - file: /etc/systemd/system/eclipse-mosquitto.service
      - file: /root/config/eclipse-mosquitto/mosquitto.conf
    - watch:
      - file: /root/config/eclipse-mosquitto/mosquitto.conf
</pre>
</div>

<p>
The Salt state also made sure the right packages and other details where ready before the service was started.
</p>

<p>
This was very simple and worked well so far. One disadvantage is that the workloads are tied to a particular Pi. I was not going to make the setup more complex by building my own orchestrator.
</p>

<p>
Another disadvantage is that I was pulling the containers into the SD card. I was not hoping for a long life of these. After it died, I took it as a good opportunity to re-do this setup.
</p>

<p>
My long term goal would be to netboot the Pi&rsquo;s, and have the storage mounted. I am not very familiar with all the procedure, so I will go step by step.
</p>

<p>
I decided for the the initial iteration:
</p>

<ul class="org-ul">
<li><a href="https://k3s.io/">k3s (Lightweight Kubernetes)</a> on the Pi&rsquo;s</li>
<li>The k3s server to use a USB disks/SSDs with <a href="https://btrfs.wiki.kernel.org/index.php/Main_Page">btrfs</a> as storage</li>
<li>The worker nodes to /var/lib/rancher/k3s from USB storage</li>
<li>Applying the states over almost stock <a href="http://download.opensuse.org/ports/aarch64/distribution/leap/15.2/appliances/">Leap 15.2</a> images should result in a working cluster</li>
</ul>

<p>
All the above managed with <span class="underline">salt-ssh</span> tree on a git repository just like I was used to
</p>

<section id="outline-container-k3s-installation" class="outline-2">
<h2 id="k3s-installation">k3s installation</h2>
<div class="outline-text-2">
<p>
We start by creating <code>k3s/init.sls</code>. For the k3s state I defined a minimal pillar defining the server and the shared token:
</p>

<div class="org-src-container">
<pre class="src src-yaml">k3s:
  token: xxxxxxxxx
  server: rpi03
</pre>
</div>

<p>
The first part of the k3s state ensures cgroups are configured correctly and disables swap:
</p>

<div class="org-src-container">
<pre class="src src-yaml">k3s.boot.cmdline:
  file.managed:
    - name: /boot/cmdline.txt
    - contents: |
	cgroup_enable=cpuset cgroup_memory=1 cgroup_enable=memory

k3s.disable.swap:
  cmd.run:
    - name: swapoff -a
    - onlyif:  swapon --noheadings --show=name,type | grep .
</pre>
</div>

<p>
As the goal was to avoid using the SD card, the next state makes sure <code>/var/lib/rancher/k3s</code> is a mount. I have to admit I wasted quite some time getting right the state for the storage mount. Using <a href="https://docs.saltstack.com/en/latest/ref/states/all/salt.states.mount.html#salt.states.mount.mounted"><code>mount.mounted</code></a> did not work because it is buggy and took different <code>btrfs</code> subvolume mounts from the same device as the same mount.
</p>

<div class="org-src-container">
<pre class="src src-yaml">k3s.volume.mount:
  mount.mounted:
    - name: /var/lib/rancher/k3s
    - device: /dev/sda1
    - mkmnt: True
    - fstype: btrfs
    - persist: False
    - opts: "subvol=/@k3s"
</pre>
</div>

<p>
I resorted then to write my own state. I discovered the awesome <a href="https://www.man7.org/linux/man-pages/man8/findmnt.8.html">findmnt</a> command, and my workaround looked like:
</p>

<div class="org-src-container">
<pre class="src src-yaml">k3s.volume.mount:
  cmd.run:
    - name: mount -t btrfs -o subvol=/@{{ grains['id'] }}-data /dev/sda1 /data
    - unless: findmnt --mountpoint /data --noheadings | grep '/dev/sda1[/@k3s]'
    - require:
	- file: k3s.volume.mntpoint
</pre>
</div>

<p>
This turned later to be a pain, as the k3s installer started k3s without caring much if this volume was mounted or not. Then I remembered: systemd does exactly that. It manages mount and dependencies. This simplified the mount state to:
</p>

<div class="org-src-container">
<pre class="src src-yaml">k3s.volume.mount:
  file.managed:
    - name: /etc/systemd/system/var-lib-rancher-k3s.mount
    - contents : |
	[Unit]

	[Install]
	RequiredBy=k3s
	RequiredBy=k3s-agent

	[Mount]
	What=/dev/sda1
	Where=/var/lib/rancher/k3s
	Options=subvol=/@k3s
	Type=btrfs
  cmd.run:
    - name: systemctl daemon-reload
    - onchanges:
	- file: k3s.volume.mount
  service.running:
    - name: var-lib-rancher-k3s.mount
</pre>
</div>

<p>
The k3s state works as follows: it runs the installation script in server or agent mode depending if the pillar <code>k3s:server</code> entry matches with the node where the state is applied.
</p>

<div class="org-src-container">
<pre class="src src-yaml">{%- set k3s_server = salt['pillar.get']('k3s:server') -%}
{%- if grains['id'] == k3s_server %}
{%- set k3s_role = 'server' -%}
{%- set k3s_suffix = "" -%}
{%- else %}
{%- set k3s_role = 'agent' -%}
{%- set k3s_suffix = '-agent' -%}
{%- endif %}

k3s.{{ k3s_role }}.install:
  cmd.run:
    - name: curl -sfL https://get.k3s.io | sh -s -
    - env:
	- INSTALL_K3S_TYPE: {{ k3s_role }}
{%- if k3s_role == 'agent' %}
	- K3S_URL: "https://{{ k3s_server }}:6443"
{%- endif %}
	- INSTALL_K3S_SKIP_ENABLE: "true"
	- INSTALL_K3S_SKIP_START: "true"
	- K3S_TOKEN: {{ salt['pillar.get']('k3s:token', {}) }}
    - unless:
	# Run install on these failed conditions
	# No binary
	- ls /usr/local/bin/k3s
	# Token changed/missing
	- grep '{{ salt['pillar.get']('k3s:token', {}) }}' /etc/systemd/system/k3s{{ k3s_suffix }}.service.env
	# Changed/missing server
{%- if k3s_role == 'agent' %}
	- grep 'K3S_URL=https://{{ k3s_server }}:6443' /etc/systemd/system/k3s{{ k3s_suffix }}.service.env
{%- endif %}
    - require:
	- service: k3s.volume.mount
	- service: k3s.kubelet.volume.mount

k3s.{{ k3s_role }}.running:
  service.running:
    - name: k3s{{ k3s_suffix }}
    - enable: True
    - require:
      - cmd: k3s.{{ k3s_role }}.install
</pre>
</div>
</div>
</section>

<section id="outline-container-workloads" class="outline-2">
<h2 id="workloads">Workloads</h2>
<div class="outline-text-2">
<p>
The next step is to move workloads like <a href="https://www.home-assistant.io">homeassistant</a> into this setup.
</p>

<p>
k3s allows to automatically deploy manifests located in <code>/var/lib/rancher/server/manifests</code>. We can deploy eg. mosquitto like the following:
</p>

<div class="org-src-container">
<pre class="src src-yaml">homeassistant.mosquitto:
  file.managed:
    - name: /var/lib/rancher/k3s/server/manifests/mosquitto.yml
    - source: salt://homeassistant/files/mosquitto.yml
    - require:
      - k3s.volume.mount
</pre>
</div>

<p>
With <code>mosquito.yml</code> being:
</p>

<div class="org-src-container">
<pre class="src src-yaml">---
apiVersion: v1
kind: Namespace
metadata:
  name: homeassistant
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mosquitto
  namespace: homeassistant
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mosquitto
  template:
    metadata:
      labels:
	app: mosquitto
    spec:
      containers:
	- name: mosquitto
	  image: docker.io/library/eclipse-mosquitto
	  resources:
	    requests:
	      memory: "64Mi"
	      cpu: "100m"
	    limits:
	      memory: "128Mi"
	      cpu: "500m"
	  ports:
	  - containerPort: 1883
	  imagePullPolicy: Always
---
apiVersion: v1
kind: Service
metadata:
  name: mosquitto
  namespace: homeassistant
spec:
  ports:
  - name: mqtt
    port: 1883
    targetPort: 1883
    protocol: TCP
  selector:
    app: mosquitto
</pre>
</div>

<p>
Homeassistant is no different, except that we use a <a href="https://kubernetes.io/docs/concepts/configuration/configmap/">ConfigMap</a> resource to store the configuration and define an <a href="https://docs.traefik.io/providers/kubernetes-ingress/">Ingress</a> resource to access it from the LAN:
</p>

<div class="org-src-container">
<pre class="src src-yaml">---
apiVersion: v1
kind: Namespace
metadata:
  name: homeassistant
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: homeassistant-config
  namespace: homeassistant
data:
  configuration.yaml: |
    homeassistant:
      auth_providers:
	- type: homeassistant
	- type: trusted_networks
	  trusted_networks:
	    - 192.168.178.0/24
	    - 10.0.0.0/8
	    - fd00::/8
	  allow_bypass_login: true
      name: Home
      latitude: xx.xxxx
      longitude: xx.xxxx
      elevation: xxx
      unit_system: metric
      time_zone: Europe/Berlin
    frontend:
    config:
    http:
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: homeassistant
  namespace: homeassistant
spec:
  replicas: 1
  selector:
    matchLabels:
      app: homeassistant
  template:
    metadata:
      labels:
	app: homeassistant
    spec:
      containers:
	- name: homeassistant
	  image: homeassistant/raspberrypi3-64-homeassistant:stable
	  volumeMounts:
	    - name: config-volume-configuration
	      mountPath: /config/configuration.yaml
	      subPath: configuration.yaml
	  livenessProbe:
	    httpGet:
	      scheme: HTTP
	      path: /
	      port: 8123
	    initialDelaySeconds: 30
	    timeoutSeconds: 30
	  resources:
	    requests:
	      memory: "512Mi"
	      cpu: "100m"
	    limits:
	      memory: "1024Mi"
	      cpu: "500m"
	  ports:
	    - containerPort: 8123
	      protocol: TCP
	  imagePullPolicy: Always
      volumes:
	- name: config-volume-configuration
	  configMap:
	    name: homeassistant-config
	    items:
	    - key: configuration.yaml
	      path: configuration.yaml
---
apiVersion: v1
kind: Service
metadata:
  name: homeassistant
  namespace: homeassistant
spec:
  selector:
    app: homeassistant
  ports:
    - port: 8123
      targetPort: 8123
---
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: homeassistant
  namespace: homeassistant
  annotations:
    kubernetes.io/ingress.class: traefik
    traefik.frontend.rule.type: PathPrefixStrip
spec:
  rules:
  - host: homeassistant.int.mydomain.com
    http:
      paths:
      - path: /
	backend:
	  serviceName: homeassistant
	  servicePort: 8123
</pre>
</div>

<p>
Setting up Ingress was the most time consuming part. It took me a while to figure out how it was supposed to work, and customizing the <a href="https://docs.traefik.io/">Treafik</a> Helm chart is not intuitive to me. While homeassistant was more straightforward as it is a simple HTTP behind SSL proxy service, the Kubernetes dashboard is already deployed with SSL inside the cluster. I am still figuring out how <code>ingress.kubernetes.io/protocol: https</code>, <code>traefik.ingress.kubernetes.io/pass-tls-cert: "true"</code> (oh, don&rsquo;t forget the quotes!) or <code>insecureSkipVerify</code> work toghether and what is the best way to expose it to the LAN.
</p>

<p>
In a future post, I will describe the dashboards setup, and other improvements.
</p>
</div>
</section>

</div>

 </main>

<hr/>
 
 <p class='disclaimer'>The postings on this site are my own and don't necessarily represent my employer’s positions, strategies or opinions.</p>

  <p>Last updated 06 Jan 2025. Built with <a href="https://www.gnu.org/software/emacs/">Emacs</a> undefined (<a href="https://orgmode.org/">Org</a> mode undefined). <a href="http://localhost:8000/readme.html">Details</a>.</p>
</body>
</html>
